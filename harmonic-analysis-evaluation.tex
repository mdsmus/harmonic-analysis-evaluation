\documentclass{article}
\usepackage{ismir}
\usepackage{amsmath}
\usepackage{graphicx}
\usepackage{url}
\usepackage[utf8x]{inputenc}
\usepackage[T1]{fontenc}
\usepackage[english]{babel}
%\usepackage[htt]{hyphenat}
\usepackage{times}
\usepackage{color}

\newcounter{notecounter}

\newcommand{\note}[1]{
  \addtocounter{notecounter}{1}
  \textcolor{red}{[note \arabic{notecounter}: #1]}
}

\newcommand{\comment}[1]{}

\title{An evaluation of heuristic and machine learning algorithms for
  symbolic chord finding} \oneauthor {}{}
%% Teoricamente é para não botar o nome do autor no artigo, por causa
%% do processo de revisão.



\begin{document}
\graphicspath{{figs/}{data/}}
\maketitle

\begin{abstract}
Chord finding is an important subset of harmonic analysis. Many
algorithms have been proposed, but no careful comparison between them
exists. Viewing chord finding as an information retrieval problem
might bring some insights into understanding and improving current
techniques. Here we evaluate Tsui's neural networks, Pardo and
Birmingham's HarmAn and Maxwell's expert system under this light. A
k-nearest-neighbor algorithm considering surrounding context of a
sonority has the best averaged precision and recall over all chord
modes, while a neural network that looks only at a single sonority
is the best overall chord labeler.
\end{abstract}

\section{Introduction}
\label{sec:introduction}

\comment{
  ==> chord finding é importante, vários algoritmos propostos, poucas
  comparações 
  ==> revisar algoritmos
  ==> descrever artigo
}

Chords are useful metadata. When an analyst perform harmonic analysis
of a piece of tonal music he starts by noting where and what the
possible chords are. Then, he uses this information to find the
piece's tonality, cadence, modulations and overall structure. After
analysis he can use this acquired information as he wishes, to, for
example, understand the harmony of that piece, find other similar
songs, or write an accompaniment. Finding the chords of a piece of
tonal music is just metadata extraction, and the study of chord
finding as such can increse understanding of the performance and
limitations of current techniques.

The problem of automated finding the chord structure of symbolic
scores has been approached from many directions, mostly mirroring
approaches to natural language processing currently in fashion.
First, Winograd \cite{winograd:linguistics} and Ulrich
\cite{ulrich:analysis} developed backtracking parsers based of formal
grammars and productions. After a hiatus they were followed by
Maxwell's \cite{maxwell:expert} production rule-based expert system
and Temperley and Sleator's Melisma \cite{temperley.ea:modeling},
based on preference rules. These were followed by Pardo and
Birmingham's HarmAn \cite{pardo.ea:automated}, Barthelemy and Bonardi
\cite{barthelemy.ea:figured} and Taube's workbench
\cite{taube:automatic}, all built using pattern-matching as their core
chord-finding method. Following these are Tsui's \cite{tsui:harmonic}
neural network algorithm and Temperley's \cite{temperley:bayesian}
bayesian approach.

Here we present an evaluation Pardo and Birmingham's, Maxwell's,
Tsui's and two orignal algorithms. First, in section
\ref{sec:methodology} we discuss our evaluation methodology. In
section \ref{sec:algorithms} we present Pardo et al's HarmAn,
Maxwell's algorithms, Tsui's artificial neural networks, our decision
trees and a baseline k-nearest-neighbours method. We compare their
overall performance in section \ref{sec:discussion} and state our case
in section \ref{sec:conclusions}.

\section{Methodology}
\label{sec:methodology}

\comment{
  ==> um problema de information retrieval
  ==> precision, recall (e significado)
  ==> corais de bach
}

All these techniques and methods were described informally, and very
little source code is avaliable for testing. Some articles, most
notably Barthelemy and Bonardi \cite{barthelemy.ea:figured} and
Temperley and Sleator \cite{temperley.ea:modeling}, don't provide
enough information to reproduce their results reliably. Every
benchmark found in literature \cite{pardo.ea:automated,
  barthelemy.ea:figured, tsui:harmonic, taube:automatic,
  illescas.ea:harmonic} is only based on published examples, and this
difficults a thorough statistical evaluation of each technique's main
merits and flaws.

Chord finding in a tonal piece is definitely not trivial. While it is
possible to make a good guess just by looking at the notes in a
sonority, this guess is often wrong, and it is impossible to tell
exactly how it is wrong without looking at the surrounding
context. Also, many times a sonority might seem to form a chord, but
that chord could have no tonal function in that part of the piece,
and, instead, one or more of those notes are there for purely melodic
purposes. For example, figure \note{passos: botar figura do exemplo
  v->acorde que parece diminuto->v7->i} shows a dominant chord that,
in passing to a dominant chord with minor seventh, forms a
configuration similar to that of a diminished chord. Therefore, some
form of contextual information is necessary to correctly label a
chord.

A chord finding algorithm has to decide which of the possible
labelings is best for a given sonority. Evaluating its performance
with a simple metric such as the ratio of correct classifications
might be useful to give an idea of how well it generally works, but
hides many defficiencies in the less common chord types. For example,
correctly labeling major chords, minor chords and non-chord tones in
our corpus is enough to get a $77\%$ accuracy. Also, some chord modes
are often mistaken for others (for example, a minor seventh chord
might have the same notes as a major chord) and some algorithms tend
to ignore less frequent chord modes. For these reasons we propose a
different evaluation methodology. We will use a baseline algorithm,
k-nearest-neighbours, to assess how much can be done by just
remembering previous classifications. We will also evaluate our
algorithms by their precision and recall with respect to each chord
mode, thereby showing where each most easily fails; and we will
compute the bias of each algorithm regarding each chord mode.

We will perform these evaluation on a corpus of 140 analyzed Bach
Chorales from the Riemenschneider edition \cite{bach:371}. The code
and data used in this evaluation are availiable, together with
instructions to reproduce our analysis, at the \texttt{ismir2008}
branch of a git \cite{baudis:git} repository at
\url{git://genos.mus.br/rameau.git}.

\section{Algorithms}
\label{sec:algorithms}

\comment{
  ==> features usadas pelos algoritmos
  ==> técnicas variadas: busca X learning
}

We study two main approaches to chord finding in this article:
heuristic algorithms and machine learning techniques. A heuristic
algorithm works by using some set of rules (generated by an expert on
the subject) to find the best possible labeling of a sonority, while
machine learning techniques employ statistical correlations found in
examples of correct labelings to induce a model of harmonic
analysis. Machine learning algorithms have been proved superior to
more complex models at some areas of Music Information Retrieval, such
as tonality detection \cite{gomez.ea:estimating}.

All these algorithms, however, share the characteristic that they
operate on a set of features extracted from the data. This is done for
varied reasons, such as simplifying the algorithm, reducing the search
space or preserving important invariances in the algorithm. The
selection of appropriate features might be the difference between an
intractable problem and a trivial one \cite{duda.ea:statistical}.


\subsection{Pardo and Birmingham's algorithm}
\label{sec:pardo}

\comment{
  ==> algoritmo descrito em \cite{pardo.ea:algorithms}
  ==> baseado em templates e pattern matching
  ==> ignora enarmonia
  ==> feature é um conjunto de pitches
  ==> nossa extensão:
  ===> mais templates
  ===> enarmonia
  ==> regras de desempate: a princípio promissoras, mas a abordagem
  não escala bem
}

Pardo and Birmingham \cite{pardo.ea:algorithms} describe a
pattern-matching algorithm for chordal analysis based on
templates. The article enumerates six such templates, and the
algorithm searches across the piece for the labeling that better
matches the notes found. When a tie happens between two templates, a
tie-breaking heuristic is used (one such heuristic is ``prefer more
common labelings''). In practice, most of the non-trivial decisions
made by this algorithm are codified as tie-breaking rules, and this
approach doesn't scale well.

Some limiations are obvious from the article. The algorithm, for
example, has no notion of a minor chord with a minor seventh, or an
augmented chord, or a chord without a third (and, consequently, having
a melodic purpose on the piece). Also, since their system ignores
enharmonic information some fine distinctions are lost, and a german
augmented sixth is shown being classified as a dominant seventh.

We have extended the algorithm presented in the article by
incorporating more chord templates (giving a total of 10 chord
templates) and giving it access to enharmonic information, which
improved the recall of fully diminished chords and the precision in
recognizing major chords. We have also added a bass note
detection. The original algorithm is referred to as \texttt{s-pb} and
our extended version as \texttt{es-pb}.

\subsection{Maxwell's expert system}
\label{sec:maxwell}


\subsection{Decision Trees}
\label{sec:tree}

\comment{
  ==> ID3 algorithm
  ==> features: sequência de pitches
  ==> dependência em quatro vozes 
}

Our decision trees use the ID3 algorithm
\cite{mitchell:machine}. Since our decision tree library can't handle
numeric attributes well, the features we chose were the four pitch
classes of each sonority to be classified. This, unfortunately,
makes it impossible for these algorithms to generalize to musical
styles not very similar to a four-part chorale.

We have implemented two decision trees: \texttt{s-tree} and
\texttt{es-tree}. The only difference between them is that
\texttt{es-tree} distinguishes enharmonic notes.

\subsection{Artificial Neural Networks}
\label{sec:neural-net}

\comment{
  ==> features são weighted pitch counts
  ==> simple/enharmonic-simple/context
  ==> mostrar gráfico de hidden units
}

The artificial neural networks we used are modeled after Tsui's Root
Network A \cite{tsui:harmonic}. The main differences are in the
feature extraction process and that we consider more chord types than
major and minor. Tsui \cite{tsui:harmonic} uses as features a vector,
each position representing one pitch class. On training, he first
transposes every sonority to the 12 possible pitch classes to ensure
that the network is transposition-invariant. While interesting, this
approach becomes computationally prohibitive if one wants to
distinguish enharmonic notes. Instead, we chose to transpose each
sonority so that it has C as its bass note. This codification has
interesting properties and has no accuracy burden.

We have three neural network algorithms: \texttt{s-net},
\texttt{es-net} and \texttt{ec-net}. \texttt{s-net} looks at one
sonority at a time and discards enharmonic information. \texttt{es-net}
looks at one sonority at a time but distinguishes enharmonic
notes. \texttt{ec-net} is equivalent to \texttt{es-net}, but looks
also at the surrounding context of a sonority. The number of hidden
units on the neural networks were determined by benchmarking their
performance against a separate validation set. The results are in
figure \ref{fig:hidden-units}

\begin{figure}
  \centering
  \includegraphics[scale=0.35,angle=270]{neural-net-train}
  \caption{The number of hidden units in each network}
  \label{fig:hidden-units}
\end{figure}


\subsection{K-Nearest-Neighbors}
\label{sec:knn}

\comment{
  ==> baseline (similar a pardo, templates automaticos)
  ==> good theoretical performance (citar \cite{fix.ea:important}) 
  ==> very good practical performance (citar
  \cite{gomez.ea:estimating})
  ==> dizer valor de k, peso do contexto
}

Due to its good performance \cite{fix.ea:important,
  gomez.ea:estimating} and simple model we have adopted the
k-nearest-neighbours as a baseline algorithm. The features used in
this classifier were the same as in the neural networks. We have two
k-nearest-neighbor algorithms, \texttt{es-knn} and
\texttt{ec-knn}. They both distinguish enharmonic notes. The only
difference between them is that \texttt{es-knn} looks at one sonority
at a time, while \texttt{ec-knn} also receives information from
surrounding sonorities as input. Experimentally we have determined
that the best k value is 1, using only the nearest neighbor, for
\texttt{es-knn} and 2 for \texttt{ec-knn}. The surrounding sonorities
in ec-knn are weighted but a factor of 2 times their distance from the
current sonority.

\section{Discussion}
\label{sec:discussion}

\begin{table}
  \centering
  \begin{tabular}{l|p{.5cm}p{.5cm}p{.5cm}p{.5cm}p{.5cm}p{.5cm}p{.5cm}p{.5cm}p{.5cm}}
   & EC-Knn&EC-net &ES-Knn &ES-net &ES-PB  &ES-tree&S-net  &S-PB   &S-tree \\
\hline                                               
M  &$93.7 $&$ 94.1$&$ 93.5$&$ 97.1$&$ 88.7$&$ 86.3$&$ 97.1$&$ 67.4$&$86.5$   \\
M7 &$84.3 $&$ 79.7$&$ 85.0$&$ 93.9$&$ 73.8$&$ 79.5$&$ 89.7$&$ 73.3$&$79.2$   \\
M7+&$31.7 $&$ 38.1$&$ 28.6$&$ 45.4$&$ 40.1$&$ 24.8$&$ 46.1$&$~~0.0$&$25.6$   \\
m  &$88.7 $&$ 88.6$&$ 91.1$&$ 92.9$&$ 79.9$&$ 83.0$&$ 91.8$&$ 79.9$&$83.9$   \\
m7 &$72.9 $&$ 69.9$&$ 73.0$&$ 72.9$&$ 83.9$&$ 68.8$&$ 82.5$&$~~0.0$&$64.7$   \\
°  &$71.2 $&$ 55.7$&$ 72.5$&$ 76.2$&$ 69.1$&$ 56.7$&$ 73.9$&$ 66.8$&$50.2$   \\
°7 &$67.4 $&$ 47.1$&$ 73.6$&$ 65.6$&$ 59.7$&$ 64.8$&$ 68.8$&$ 68.8$&$66.7$   \\
ø  &$74.3 $&$ 52.3$&$ 83.4$&$ 77.8$&$ 42.1$&$ 79.4$&$ 77.2$&$ 42.0$&$79.0$   \\
+  &$57.1 $&$ 40.0$&$~~0.0$&$~~0.0$&$ 31.8$&$~~0.0$&$~~0.0$&$~~0.0$&$~~0.$   \\
!  &$20.0 $&$~~9.1$&$ 29.6$&$ 27.0$&$~~4.8$&$ 35.3$&$ 29.2$&$~~0.0$&$35.3$   \\
nct&$88.4 $&$ 87.4$&$ 89.6$&$ 91.7$&$~~0.0$&$ 79.7$&$ 90.9$&$~~0.0$&$87.0$   \\
avg&$68.2 $&$ 60.2$&$ 65.4$&$ 67.3$&$ 52.2$&$ 59.8$&$ 67.9$&$ 36.2$&$59.8$   \\

  \end{tabular}

\medskip

nct: non-chord tone, !: incomplete chord

  \caption{Precision (\%)}
  \label{tab:precision}
\end{table}

\begin{table}
  \centering
  \begin{tabular}{l|p{.5cm}p{.5cm}p{.5cm}p{.5cm}p{.5cm}p{.5cm}p{.5cm}p{.5cm}p{.5cm}}

   &EC-Knn&EC-net &ES-Knn &ES-net &ES-PB  &ES-tree&S-net  &S-PB   &S-tree \\
\hline                                            
M  &$95.7$&$ 93.3$&$ 96.4$&$95.7 $&$ 95.2$&$ 94.2$&$96.3 $&$ 97.7$&$  95.2$ \\
M7 &$94.8$&$ 85.7$&$ 94.4$&$92.5 $&$ 95.1$&$ 77.3$&$94.9 $&$ 95.3$&$  77.3$ \\
M7+&$62.2$&$ 73.5$&$ 60.9$&$95.5 $&$ 96.8$&$ 63.7$&$95.5 $&$  0.0$&$  66.2$ \\
m  &$92.4$&$ 90.4$&$ 92.9$&$95.4 $&$ 95.9$&$ 88.0$&$95.8 $&$ 95.8$&$  87.5$ \\
m7 &$82.1$&$ 77.4$&$ 88.6$&$88.0 $&$ 77.2$&$ 64.1$&$79.9 $&$~~0.0$&$  65.2$ \\
°  &$88.1$&$ 70.9$&$ 91.7$&$90.4 $&$ 89.5$&$ 55.9$&$91.0 $&$ 90.3$&$  68.9$ \\
°7 &$71.6$&$ 69.1$&$ 82.7$&$98.8 $&$ 98.8$&$ 56.8$&$95.1 $&$ 95.1$&$  59.3$ \\
ø  &$50.8$&$ 29.9$&$ 57.5$&$71.9 $&$ 81.6$&$ 46.2$&$72.5 $&$ 81.6$&$  46.4$ \\
+  &$25.0$&$ 14.3$&$~~0.0$&$~~0.0$&$ 82.4$&$~~0.0$&$~~0.0$&$~~0.0$&$ ~~0.0$ \\
!  &$14.6$&$~~4.7$&$ 19.0$&$57.1 $&$ 62.8$&$ 14.0$&$17.5 $&$~~0.0$&$  14.0$ \\
nct&$67.0$&$ 73.3$&$ 64.8$&$74.5 $&$~~0.0$&$ 60.2$&$76.8 $&$~~0.0$&$  57.8$ \\
avg&$88.4$&$ 87.4$&$ 89.6$&$78.2 $&$~~0.0$&$ 79.7$&$74.1 $&$~~0.0$&$  87.0$ \\
  \end{tabular}                                                        

\medskip

nct: non-chord-tone, !: incomplete chord
  \caption{Recall (\%)}
  \label{tab:recall}
\end{table}


\begin{table}
  \centering
  \begin{tabular}{l|p{.5cm}p{.5cm}}
       & $\%$     & $\sigma$ \\
\hline
es-net &$   90  $&$ 10$   \\
es-knn &$   87  $&$~~9$   \\
ec-knn &$   86  $&$ 10$   \\
ec-net &$   84  $&$ 10$   \\ 
s-net  &$   83  $&$ 14$   \\
es-tree&$   79  $&$ 11$   \\
es-pb  &$   75  $&$ 10$   \\
s-tree &$   74  $&$ 15$   \\
s-pb   &$   64  $&$ 14$   \\

  \end{tabular}                                                        

\medskip

nct: non-chord-tone, !: incomplete chord
  \caption{Accuracy (\%)}
  \label{tab:accuracy}
\end{table}

\section{Conclusions}
\label{sec:conclusions}


\bibliographystyle{plain}
\bibliography{strings-short,ismir,programs,coding,harmonic-analysis,dont-have,artifical-inteligence,music-harmony-and-theory,licenses,icmc}

\end{document}

