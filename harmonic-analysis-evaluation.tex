\documentclass{article}
\usepackage{ismir}
\usepackage{amsmath}
\usepackage{graphicx}
\usepackage{url}
\usepackage[utf8x]{inputenc}
\usepackage[T1]{fontenc}
\usepackage[english]{babel}
%\usepackage[htt]{hyphenat}
\usepackage{times}
\usepackage{color}
\usepackage[displaymath,textmath,sections,graphics,floats,auctex]{preview}

\newcounter{notecounter}

\newcommand{\note}[1]{
  \addtocounter{notecounter}{1}
  \textcolor{red}{[note \arabic{notecounter}: #1]}
}
\newcommand{\comment}[1]{}

\title{An evaluation of some algorithms for symbolic chord labeling}
\oneauthor {}{}
%% Teoricamente é para não botar o nome do autor no artigo, por causa
%% do processo de revisão.

\PreviewEnvironment{itemize}
\PreviewEnvironment{enumerate}


\begin{document}
\graphicspath{{figs/}{data/}}
\maketitle

\begin{abstract}

  Chord labeling is an important part of harmonic analysis. Many
  algorithms have been proposed for it, but there is no objective
  comparison of their main merits and flaws. In this paper we present
  an evaluation of chord labeling algorithms on 18 Bach's chorales,
  comparing Pardo and Birmingham's and Tsui's algorithms and
  evaluating two new proposals: a k-nearest-neighbors and a decision
  tree classifiers.
  
  Our evaluation methodology tries to be as precise as possible in
  enumerating the errors made by each algorithm. We split the overall
  accuracy in precision and recall, and note which chord types are
  most often mistaken by others. This highlights the difficult areas
  and suggest, for example, how to better extend our training
  corpus. 
  
  We find that Tsui's neural network classifier without contextual
  information is the best labeler for all chord modes in our corpus,
  and enhancing it with the surrounding context is the best way of
  incorporating contextual information in the decision process. Pardo
  and Birmingham's algorithm has a very good recall, but a bad
  precision, probably due to its ignorance of non-chord tones, and is
  outperformed by the k-nearest-neighbors classifier, also a pattern
  matcher. The decision tree's performance in unremarkable.
\end{abstract}

\section{Introduction}
\label{sec:introduction}

Chord labeling consists in assigning a name to a set of notes. The
process involves dividing a musical score in symbolic format in
different sonorities, deciding which notes in a sonority are part of a
chord, and naming the sonorities with symbols like E$\flat$ or
A$\sharp$7M.

The problem of automatic chord labeling of symbolic scores has been
approached from many directions since the sixties. First, Winograd
\cite{winograd68:linguistics} and Ulrich \cite{ulrich77:analysis}
developed backtracking parsers based of formal grammars and
productions. After a hiatus they were followed by Maxwell's
\cite{maxwell92:expert} rule-based expert system and Temperley and
Sleator's preference-rule Melisma system
\cite{temperley.ea99:modeling}. By the end of the nineties Pardo and
Birmingham's HarmAn \cite{barthelemy.ea01:figured}, Barthelemy and
Bonardi's algorithm \cite{pardo.ea02:algorithms}, and Taube's
workbench \cite{taube99:automatic} were built, all using
pattern-matching as their core chord-finding method. Parallel to these
are Tsui's neural network algorithm \cite{tsui02:harmonic}, Temperley's
bayesian approach \cite{temperley04:bayesian}, and Raphael and
Stoddard's hidden Markov model \cite{raphael.ea03:harmonic}. Recently,
Illescas et al. designed a mixed system, based on rules and search
through a graph of possible solutions \cite{illescas.ea07:harmonic}.

Most of these techniques and methods are not described rigorously, and
very little source code is available for testing, which is a problem
since some articles, most notably Barthelemy and Bonardi
\cite{pardo.ea02:algorithms} and Temperley and Sleator
\cite{temperley.ea99:modeling}, don't provide enough information to
reproduce their results reliably. Every benchmark found in literature
\cite{pardo.ea00:automated, pardo.ea02:algorithms, tsui02:harmonic,
  taube99:automatic, illescas.ea07:harmonic} is based only on
published examples, and this makes difficult a thorough statistical
evaluation of the main merits and flaws of each technique.

To correct this defficiency we have reimplemented and evaluated some
chord labeling algorithms in a precise statistical framework. This
evaluation is performed on Bach Chorales, and we describe our
methodology in section \ref{sec:methodology}. The algorithms we
reimplemented (and the algorithms we propose in this paper) are
described in section \ref{sec:algorithms}. We compare their overall
performance in section \ref{sec:discussion} and state our case in
section \ref{sec:conclusions}.

\begin{table}
\centering
\begin{tabular}{r|l|r}
chord & meaning                   & count\\ \hline
    M & major chord               & 773 \\
   M7 & major chord, minor seventh& 182 \\
  M7+ & major chord, major seventh& 20 \\
    m & minor chord               & 311 \\
   m7 & minor chord, minor seventh& 109 \\
    ° & diminished triad          & 91 \\
   °7 & fully diminished chord    & 30 \\
   ø7 & half-diminished chord     & 44  \\
  aug & augmented chord           & 2   \\
  inc & incomplete chord          & 4   \\
 aug6 & augmented sixth chord     & 3   \\
  nct & non-chord tone            & 410 \\
\end{tabular}
\caption{Chord labels used in this article and their occurrences in our
corpus}
\label{tab:legenda}
\end{table}

\section{Methodology}
\label{sec:methodology}

\begin{table*}
\centering
\begin{tabular}{l||r|r|r|r|r|r|r|r|r|r|r|r|r}
      &     M &    M7 &   M7+ &     m &    m7 &    ° &   °7 &   ø7 &   aug &   inc &  aug6 &   nct \\  \hline \hline
    M & $ 774 $ & $     $ & $     $ & $   1 $ & $   1 $ & $     $ & $     $ & $     $ & $     $ & $     $ & $     $ & $  34 $ \\ \hline
   M7 & $     $ & $ 176 $ & $     $ & $   2 $ & $     $ & $   7 $ & $     $ & $     $ & $     $ & $     $ & $     $ & $  12 $ \\ \hline
  M7+ & $  11 $ & $     $ & $   1 $ & $     $ & $     $ & $     $ & $     $ & $     $ & $     $ & $     $ & $     $ & $  38 $ \\ \hline
    m & $   3 $ & $     $ & $   1 $ & $ 313 $ & $   4 $ & $     $ & $     $ & $     $ & $     $ & $     $ & $     $ & $   7 $ \\ \hline
   m7 & $   5 $ & $   4 $ & $     $ & $     $ & $ 106 $ & $     $ & $     $ & $     $ & $     $ & $     $ & $     $ & $   5 $ \\ \hline
   ° & $     $ & $     $ & $     $ & $     $ & $     $ & $ 100 $ & $   2 $ & $     $ & $     $ & $     $ & $     $ & $   2 $ \\ \hline
  °7 & $     $ & $     $ & $     $ & $     $ & $     $ & $     $ & $  30 $ & $     $ & $     $ & $     $ & $     $ & $     $ \\ \hline
  ø7 & $     $ & $     $ & $     $ & $   1 $ & $   6 $ & $     $ & $     $ & $  44 $ & $     $ & $     $ & $     $ & $     $ \\ \hline
  aug & $     $ & $     $ & $     $ & $     $ & $     $ & $     $ & $     $ & $     $ & $     $ & $     $ & $     $ & $   4 $ \\ \hline
  inc & $   5 $ & $     $ & $     $ & $     $ & $     $ & $     $ & $     $ & $     $ & $     $ & $     $ & $     $ & $   4 $ \\ \hline
 aug6 & $     $ & $     $ & $     $ & $     $ & $     $ & $     $ & $     $ & $     $ & $     $ & $     $ & $   3 $ & $     $ \\ \hline
  nct & $  27 $ & $  21 $ & $     $ & $  40 $ & $  44 $ & $  14 $ & $   4 $ & $  13 $ & $     $ & $     $ & $     $ & $ 304 $ \\ \hline
\end{tabular}
\caption{Classifications made by our best algorithm, \texttt{ES-net}. The rows represent
  the expected answers while the columns are the returned
  results. Note that the matrix is not symmetric.}
\label{tab:erros-ES-net}
\end{table*}

The main thesis of the Heuristics and Biases program in cognitive
psychology is that the mistakes made by a person or algorithm are more
revealing of their behavior and its mode of functioning than its
correct judgments \cite{gilovich.ea02:heuristics}. This thesis has
been proved successful, and has helped detect and explain a
substantial portion of human behavior, highlighting the biases
inherent in the heuristics most people use to perform their day-to-day
activities. Here we propose a similar analysis with the intention of
facilitating detection of common biases in chord labeling
algorithms. A few biases can be readily seen in tables
\ref{tab:erros-ES-net} and \ref{tab:erros-ES-pb}, which show how some
algorithms ignore less frequent chord modes and others display clear
biases, for example labeling most major chords with a major seventh as
non-chord tones.

To concisely show most of the errors made by these algorithms we
propose a different evaluation methodology: each algorithm will be
evaluated by their precision and recall with respect to each chord
mode, highlighting many weaknesses. This decision is a bit unusual,
and deserves justification. Here, we assume that the aim of our
algorithms is to retrieve correctly each sonority belonging to a
certain chord type from a song. We also treat the detection of each
chord type separately. Hence, every time a diminished chord is
misclassified as a non chord tone, for example, the recall of
diminished chords gets smaller, while the precision of the non-chord
tones get smaller. As shown is section \ref{sec:discussion}, Pardo and
Birmingham's algorithm has a high recall and a small precision,
probably due to its incapacity of handling non-chord tones; and the
machine learning algorithm are mostly biased against less frequent
chord types.

We will perform this evaluation on a corpus of 18 Bach chorales from
the Riemenschneider edition \cite{bach41:371}, consisting of the
chorales numbered 7, 8, 10, 12, 14, 17, 18, 19, 20, 21, 22, 23, 24,
25, 26, 27, 28, 29, 30, 31, 32, 33, 36, 40, 140, 162, and 340. We
wrote answer sheets for each chorale with the chord label for each
segment. With these answer sheets, our system is able to automatically
compare the results of each algorithm with the correct result as
analyzed by musicians. The code and data used in this evaluation,
together with instructions to reproduce our analysis, are available
on-line at \url{http://removed-for-anonymity}.

\section{Algorithms}
\label{sec:algorithms}

\begin{table*}
\centering
\begin{tabular}{l||r|r|r|r|r|r|r|r|r|r|r|r|r}
      &     M &    M7 &   M7+ &     m &    m7 &    ° &   °7 &   ø7 &   aug &   inc &  aug6 &   nct \\  \hline \hline
    M & $ 774 $ & $     $ & $   3 $ & $   5 $ & $     $ & $     $ & $     $ & $   3 $ & $     $ & $  55 $ & $     $ & $     $ \\ \hline
   M7 & $     $ & $ 180 $ & $   2 $ & $   5 $ & $     $ & $   7 $ & $     $ & $     $ & $     $ & $   4 $ & $     $ & $     $ \\ \hline
  M7+ & $     $ & $     $ & $  27 $ & $   4 $ & $     $ & $     $ & $     $ & $     $ & $     $ & $     $ & $     $ & $     $ \\ \hline
    m & $   3 $ & $     $ & $   1 $ & $ 317 $ & $   2 $ & $     $ & $     $ & $   2 $ & $     $ & $   3 $ & $     $ & $     $ \\ \hline
   m7 & $   5 $ & $   4 $ & $     $ & $   5 $ & $  88 $ & $     $ & $     $ & $  39 $ & $     $ & $     $ & $     $ & $     $ \\ \hline
   ° & $   2 $ & $     $ & $     $ & $     $ & $     $ & $ 101 $ & $     $ & $     $ & $     $ & $     $ & $     $ & $     $ \\ \hline
  °7 & $     $ & $     $ & $     $ & $     $ & $     $ & $     $ & $  30 $ & $     $ & $     $ & $     $ & $     $ & $     $ \\ \hline
  ø7 & $     $ & $     $ & $     $ & $   1 $ & $     $ & $     $ & $     $ & $  46 $ & $     $ & $     $ & $   2 $ & $     $ \\ \hline
  aug & $     $ & $     $ & $     $ & $     $ & $     $ & $     $ & $     $ & $     $ & $   2 $ & $     $ & $     $ & $     $ \\ \hline
  inc & $   1 $ & $   4 $ & $     $ & $     $ & $     $ & $     $ & $     $ & $     $ & $     $ & $   2 $ & $     $ & $     $ \\ \hline
 aug6 & $     $ & $     $ & $     $ & $     $ & $     $ & $     $ & $     $ & $     $ & $     $ & $     $ & $   3 $ & $     $ \\ \hline
  nct & $ 163 $ & $ 115 $ & $  91 $ & $ 133 $ & $  28 $ & $  34 $ & $   2 $ & $  55 $ & $  10 $ & $ 138 $ & $   4 $ & $     $ \\ \hline
\end{tabular}

\caption{Classifications made by the extended Pardo and Birmingham's algorithm. The rows represent
  the expected answers while the columns are the returned
  results. Note that many chord types are ignored.}
\label{tab:erros-ES-pb}
\end{table*}

Chord labeling is a classification problem, consisting of labeling
each sonority (or sequence of sonorities) in a score with a chord
type. Most classification algorithms operate on a set of features,
that are previously extracted from the data. This is done mainly to
remove irregularities in the original inputs, reduce the
dimensionality (thereby making a problem more tractable) of the
problem, and remove noise. Each classification algorithm we present
operates on a distinct set of features, mostly derived from the
pitches of the notes. Metrical information is ignored, for now.

In this section we present Pardo and Birmingham's algorithm and a few
machine learning techniques. The machine learning algorithms were
trained on the chorales numbered 1, 2, 3, 4, 5, and 6 of the
Riemenschneider edition and on some textbook examples of augmented
sixths, augmented chords, and major chords with major sevenths
extracted from Robert Gauldin \cite{gauldin05:harmonic} and Reginald
Morris \cite{morris33:figured}. These chorales were not used in our
subsequent evaluation.

\subsection{Pardo and Birmingham's algorithm}
\label{sec:pardo}


Pardo and Birmingham \cite{barthelemy.ea01:figured} describe a
pattern-matching algorithm for chordal analysis based on templates.
The article enumerates six templates, and the algorithm searches
across the piece for the labeling that better matches the notes found.
When a tie happens between two templates, a tie-breaking heuristic is
used (one such heuristic is ``prefer more common labelings''). In
practice, most of the non-trivial decisions made by this algorithm are
codified as tie-breaking rules, and this approach doesn't scale
well. Raphael and Stoddard describe problems common to rule based
systems \cite{raphael.ea03:harmonic}, and we have found that most of
them also apply to Pardo and Birmingham's algorithm.

Some limitations are obvious from the article. The algorithm, for
example, has no notion of a minor chord with a minor seventh, or an
augmented chord, or a chord without a third. Also, since their system
ignores enharmonic information some fine distinctions are lost, and a
German augmented sixth is shown being classified erroneously as a
dominant seventh. We have then extended the algorithm presented in the
article by incorporating more chord templates (giving a total of ten)
and enabling it to distinguish enharmonic tones, which improved the
recall of fully diminished chords and the precision in recognizing
major chords, as seen in section \ref{sec:discussion}. We have also
added inversion detection. The original algorithm is referred to as
\texttt{S-pb} and our extended version as \texttt{ES-pb}.

\subsection{Decision Tree}
\label{sec:tree}

Our decision tree uses the ID3 algorithm \cite{mitchell97:machine}.
Since our decision tree library can't handle numeric attributes well,
we use the four pitch classes of each sonority to be classified as
features. This, unfortunately, makes it impossible for this algorithm
to generalize to musical styles not very similar to a four-part
chorale, although we plan on extending it to overcome this
limitation. The algorithm is named \texttt{ES-tree}. It distinguishes
enharmonic notes but ignores the surrounding context.

\subsection{Artificial Neural Networks}
\label{sec:neural-net}


The artificial neural networks we used are modeled after Tsui's Root
Network A \cite{tsui02:harmonic}. The main differences are in the
feature extraction process and that we consider chord types other than
major and minor. Tsui uses as features a vector, each position
containing how many times one pitch class sounds in a given sonority.
On training, Tsui's algorithm first transposes each sonority to the 12
possible pitch classes to ensure that the network is invariant under
transposition. While interesting, this approach becomes
computationally prohibitive if one wants to distinguish enharmonic
notes. Instead, we chose to transpose each sonority so that it has C
as its lowest note. This codification is transposition invariance and
allows a much simpler network structure.

We implemented two neural network algorithms: \texttt{ES-net}, that
does not use contextual information, and \texttt{EC-net}, which
does. Both distinguish enharmonic notes.  It should be noted that,
unlike Tsui, our neural networks have an overall worse accuracy as
context is added, as can be seen in section \ref{sec:discussion}. Our
experiments determined that the best amount of contextual information
is the least possible, so \texttt{EC-net} looks only at one preceding
and one following sonority.


\subsection{K-Nearest-Neighbors}
\label{sec:knn}

We implemented a k-nearest-neighbors algorithm because it has good
performance and is easy to implement \cite{fix.ea89:important,
  gomez.ea04:estimating}. The features used in this classifier were the
same as in the neural networks. Our two k-nearest-neighbor algorithms,
\texttt{ES-knn} and \texttt{EC-knn}, distinguish enharmonic notes, and
the only difference between them is that \texttt{ES-knn} looks at one
sonority at a time, while \texttt{EC-knn} also receives information
from surrounding sonorities as input. Experimentally we have
determined that the best k value is 1, using only the nearest
neighbor, for \texttt{ES-knn} and 2 for \texttt{EC-knn}. The
surrounding sonorities in \texttt{EC-knn} are weighted by a factor of
2 times their distance from the current sonority.

\section{Results}
\label{sec:discussion}

The results are displayed in a few tables. Tables
\ref{tab:erros-ES-net}, and \ref{tab:erros-ES-pb} display in detail
the classifications made by \texttt{ES-net} and \texttt{ES-pb},
respectively. Tables \ref{tab:precision}, \ref{tab:recall}, and
\ref{tab:f-measure} show the precision, recall and f-measure of the
evaluated algorithms. Table \ref{tab:accuracy} shows their overall
accuracy over all chords in our corpus.

\subsection{Performance by algorithm}
\label{sec:algo-perf}

The performance of Pardo and Birmingham's original algorithm,
\texttt{S-pb}, is very low on our corpus due to the variety of chord
types considered. For this reason it has the highest recall on major
chords, major chords with a minor seventh, and half-diminished chords.

The decision tree algorithm's performance is unremarkable in
comparison with the other machine learning algorithms. It displays the
smallest ability to generalize from training data, probably due to a
bad choice of features.

The contextual algorithms, \texttt{EC-knn} and \texttt{EC-net}, are
overall worse than their non-contextual equivalents, probably due to
having a smaller performance on the more common chord modes. On the
other hand, \texttt{EC-net} is the only machine learning algorithm
that recognizes augmented chords and \texttt{EC-knn}, unlike
\texttt{ES-knn}, recognizes major chords with a major seventh. As
expected, contextual information is useful in disambiguating less
frequent chord types, but might also confuse the algorithm due to the
problem known as ``the curse of dimensionality'', or the difficulty of
efficiently handling data in too many dimensions
\cite{duda.ea00:pattern}.


Overall, \texttt{ES-net} has the best precision, f-measure and
accuracy, as seen in tables \ref{tab:precision}, \ref{tab:f-measure},
and \ref{tab:accuracy}; but \texttt{ES-pb} has the best recall. As can
be seen in tables \ref{tab:erros-ES-net} and \ref{tab:erros-ES-pb},
this is most likely due to \texttt{ES-pb}'s ignorance of non-chord
tones. This suggests that an enhanced pattern matcher supporting
non-chord tones, like the one described by Taube
\cite{taube99:automatic}, can be a very good chord labeling
algorithm. This is supported by \texttt{ES-knn}'s good precision, as
seen in table \ref{tab:precision}.


\subsection{Performance by chord type}
\label{sec:chord-perf}

In this analysis we chose to separate the errors by chord type, which
can show some interesting biases. The most difficult chord type to
recognize is the incomplete chord, a chord without a third, closely
followed by the augmented chords and the augmented sixth chords. As
table \ref{tab:legenda} shows, they are the less frequent chord
types. This can cause two problems: the machine learning algorithms
have less examples to learn from and man-made heuristics tend to
either overestimate their occurrence (as seen in table
\ref{tab:precision} in the \texttt{ES-pb} column) or ignore them at
all (as seen in table \ref{tab:f-measure} on the \texttt{S-pb}
column). These biases are difficult to correct accurately.


Averaged f-measure over all chord types is, as expected, correlated
with overall accuracy, but shows some variation due to the weighting
of the chord types, as seen it table \ref{tab:accuracy}.


\begin{table}
  \centering
\begin{tabular}{l|p{0.5cm}p{0.5cm}p{0.5cm}p{0.5cm}p{0.5cm}p{0.5cm}p{0.5cm}p{0.5cm}}
     &   EC-Knn &   EC-net &   ES-Knn &   ES-net &    ES-PB &  ES-tree &   S-PB \\ \hline
   M & $  88.4$ & $  94.4$ & $  95.8$ & $  97.7$ & $  90.6$ & $  89.2$ & $  71.4$ \\
  M7 & $  85.3$ & $  91.0$ & $  89.7$ & $  94.1$ & $  75.0$ & $  82.9$ & $  74.1$ \\
 M7+ & $  50.0$ & $  25.0$ & $ ~~0.0$ & $ 100  $ & $  38.6$ & $ ~~0.0$ & $ ~~0.0$ \\
   m & $  82.7$ & $  89.5$ & $  89.4$ & $  94.6$ & $  83.9$ & $  83.8$ & $  83.6$ \\
  m7 & $  65.5$ & $  66.2$ & $  76.9$ & $  80.3$ & $  86.3$ & $  54.8$ & $ ~~0.0$ \\
  °  & $  86.0$ & $  71.0$ & $  90.0$ & $  95.2$ & $  87.1$ & $  70.4$ & $  83.5$ \\
 °7  & $  87.5$ & $  54.5$ & $ 100$   & $  90.9$ & $  96.8$ & $  51.4$ & $ 100$ \\
 ø7  & $  60.6$ & $  50.0$ & $  97.3$ & $  89.8$ & $  50.0$ & $  71.1$ & $  49.5$ \\
 aug & $ ~~0.0$ & $ 100  $ & $ ~~0.0$ & $ ~~0.0$ & $  28.6$ & $ ~~0.0$ & $ ~~0.0$ \\
 inc & $ ~~0.0$ & $ ~~0.0$ & $ ~~0.0$ & $ ~~0.0$ & $ ~~2.5$ & $ ~~0.0$ & $ ~~0.0$ \\
aug6 & $  50.0$ & $  12.5$ & $ 100  $ & $ 100  $ & $  50.0$ & $ ~~0.0$ & $ ~~0.0$ \\
 nct & $  74.1$ & $  76.6$ & $  80.8$ & $  88.3$ & $ ~~0.0$ & $  70.0$ & $ ~~0.0$ \\
\hline                                                       
avg & $   60.8$ & $  60.9$ & $  68.3$ & $  77.6$ & $  57.4$ & $  47.8$ & $  38.5$ \\
\end{tabular}


  \caption{Precision (\%)}
  \label{tab:precision}
\end{table}

\begin{table}
  \centering
\begin{tabular}{l|p{0.5cm}p{0.5cm}p{0.5cm}p{0.5cm}p{0.5cm}p{0.5cm}p{0.5cm}p{0.5cm}}
     &   EC-Knn &   EC-net &   ES-Knn &   ES-net &    ES-PB &  ES-tree &     S-PB \\ \hline
   M & $  95.2$ & $  97.3$ & $  98.3$ & $  98.7$ & $  97.2$ & $  96.3$ & $  99.7$ \\
  M7 & $  78.4$ & $  87.6$ & $  94.1$ & $  95.1$ & $  96.3$ & $  62.0$ & $  96.8$ \\
 M7+ & $  12.5$ & $ ~~4.5$ & $ ~~0.0$ & $ ~~4.5$ & $  96.4$ & $ ~~0.0$ & $ ~~0.0$ \\
   m & $  87.7$ & $  88.7$ & $  95.9$ & $  98.4$ & $  98.8$ & $  89.3$ & $  98.4$ \\
  m7 & $  66.1$ & $  78.6$ & $  83.0$ & $  94.6$ & $  77.9$ & $  56.8$ & $ ~~0.0$ \\
   ° & $  80.0$ & $  71.7$ & $  97.1$ & $  98.0$ & $  99.0$ & $  57.0$ & $  99.0$ \\
  °7 & $  70.0$ & $  20.0$ & $  70.0$ & $ 100  $ & $ 100  $ & $  60.0$ & $  86.7$ \\
  ø7 & $  42.6$ & $  21.3$ & $  76.6$ & $  93.6$ & $  97.9$ & $  56.2$ & $ 100$ \\
 aug & $ ~~0.0$ & $  50.0$ & $ ~~0.0$ & $ ~~0.0$ & $ 100  $ & $ ~~0.0$ & $ ~~0.0$ \\
 inc & $ ~~0.0$ & $ ~~0.0$ & $ ~~0.0$ & $ ~~0.0$ & $  50.0$ & $ ~~0.0$ & $ ~~0.0$ \\
aug6 & $  66.7$ & $  33.3$ & $ 100  $ & $ 100  $ & $ 100  $ & $ ~~0.0$ & $ ~~0.0$ \\
 nct & $  69.5$ & $  81.9$ & $  75.1$ & $  82.3$ & $ ~~0.0$ & $  69.8$ & $ ~~0.0$ \\
\hline                                                       
 avg & $  55.7$ & $  52.9$ & $  65.8$ & $  72.1$ & $  84.5$ & $  45.6$ & $  48.4$ \\
\end{tabular}


  \caption{Recall (\%)}
  \label{tab:recall}
\end{table}


\begin{table}
  \centering
\begin{tabular}{l|p{0.5cm}p{0.5cm}p{0.5cm}p{0.5cm}p{0.5cm}p{0.5cm}p{0.5cm}p{0.5cm}}
     &   EC-Knn &   EC-net &   ES-Knn &   ES-net &    ES-PB &  ES-tree &        S-PB \\ \hline
   M & $  91.7$ & $  95.8$ & $  97.0$ & $  98.2$ & $  93.8$ & $  92.6$ & $  83.2$ \\
  M7 & $  81.7$ & $  89.3$ & $  91.8$ & $  94.6$ & $  84.3$ & $  70.9$ & $  83.9$ \\
 M7+ & $  20.0$ & $ ~~7.6$ & $ ~~0.0$ & $ ~~8.6$ & $  55.1$ & $ ~~0.0$ & $ ~~0.0$ \\
   m & $  85.1$ & $  89.1$ & $  92.5$ & $  96.5$ & $  90.7$ & $  86.5$ & $  90.4$ \\
  m7 & $  65.8$ & $  71.9$ & $  79.8$ & $  86.9$ & $  81.9$ & $  55.8$ & $ ~~0.0$ \\
   ° & $  82.9$ & $  71.3$ & $  93.4$ & $  96.6$ & $  92.7$ & $  63.0$ & $  90.6$ \\
  °7 & $  77.8$ & $  29.3$ & $  82.4$ & $  95.2$ & $  98.4$ & $  55.4$ & $  92.9$ \\
  ø7 & $  50.0$ & $  29.9$ & $  85.7$ & $  91.7$ & $  66.2$ & $  62.8$ & $  66.2$ \\
 aug & $ ~~0.0$ & $  66.7$ & $ ~~0.0$ & $ ~~0.0$ & $  44.5$ & $ ~~0.0$ & $ ~~0.0$ \\
 inc & $ ~~0.0$ & $ ~~0.0$ & $ ~~0.0$ & $ ~~0.0$ & $ ~~4.8$ & $ ~~0.0$ & $ ~~0.0$ \\
aug6 & $  57.2$ & $  18.2$ & $ 100  $ & $ 100  $ & $  66.7$ & $ ~~0.0$ & $ ~~0.0$ \\
 nct & $  71.7$ & $  79.2$ & $  77.8$ & $  85.2$ & $ ~~0.0$ & $  69.9$ & $ ~~0.0$ \\
\hline                                                       
 avg & $  57.0$ & $  54.0$ & $  66.7$ & $  71.1$ & $  64.9$ & $  46.4$ & $  42.3$ \\
\end{tabular}


  \caption{F-measure (\%)}
  \label{tab:f-measure}
\end{table}



\begin{table}
  \centering
  \begin{tabular}{l|rrr}
       & accuracy& $\sigma$  & f-measure\\
\hline
ES-net &$   93  $&$  3$      &$71.1$ \\
ES-knn &$   90  $&$  4$      &$66.7$ \\
EC-net &$   85  $&$  5$      &$54.0$ \\
EC-knn &$   82  $&$  4$      &$57.0$ \\
ES-pb  &$   80  $&$  5$      &$64.9$ \\
ES-tree&$   79  $&$  7$      &$46.4$ \\
S-pb   &$   68  $&$ 15$      &$42.3$ \\

  \end{tabular}                                                        


  \caption{Overall Accuracy (\%)}
  \label{tab:accuracy}
\end{table}

\section{Conclusions and future work}
\label{sec:conclusions}

We have evaluated algorithms for chord labeling of symbolic scores of
tonal music on a corpus of 18 Bach chorales and have found that an
artificial neural network that does not use contextual information is
the best algorithm evaluated. We have also found that a simple
memory-based classifier can outperform a simple heuristic algorithm,
although with better heuristics this result might be reversed. We also
presented a modification to the algorithm described in
\cite{pardo.ea02:algorithms} that is demonstrably better.

We currently plan on continuing to reimplement, test, and benchmark
algorithms for automated chord labeling and harmonic analysis,
starting with Raphael and Stoddard's hidden Markov model
\cite{raphael.ea03:harmonic} and Maxwell's expert system
\cite{maxwell92:expert}.  We will extend our test corpus to
incorporate Beethoven sonatas, the Kostka-Payne corpus
\cite{temperley04:bayesian}, Bach partitas, and other classic tonal
pieces. Our aim is to benchmark and study the whole automatic harmonic
analysis process, from pitch spelling to functional and non-chord tone
analysis.

\bibliographystyle{plain}
\bibliography{strings-short,ismir,programs,coding,harmonic-analysis,dont-have,artifical-inteligence,music-harmony-and-theory,licenses,icmc,music-scores}

\end{document}

