\documentclass{article}
\usepackage{ismir}
\usepackage{amsmath}
\usepackage{graphicx}
\usepackage{url}
\usepackage[utf8x]{inputenc}
\usepackage[T1]{fontenc}
\usepackage[english]{babel}
%\usepackage[htt]{hyphenat}
\usepackage{times}
\usepackage{color}

\newcounter{notecounter}

\newcommand{\note}[1]{
  \addtocounter{notecounter}{1}
  \textcolor{red}{[note \arabic{notecounter}: #1]}
}

\title{An evaluation of algorithms for symbolic chord finding}
\oneauthor
  {Pedro Kröger, Alexandre Passos, Marcos Sampaio, Givaldo de Cidra}
  {Genos---Computer Music Research Group\\ School of Music
   \\ Federal University of Bahia, Brazil \\
  \url{{pedro.kroger,alexandre.tp,mdsmus,givaldodecidra}@gmail.com}}


\begin{document}
\graphicspath{{figs/}{data/}}
\maketitle

\begin{abstract}
\end{abstract}

\section{Introduction}
\label{sec:introduction}

Chords are useful metadata. When an analyst perform harmonic analysis
of a piece of tonal music he starts by noting where and what the
possible chords are. Then, he uses this information to find the
piece's tonality, cadence, modulations and overall structure. After
analysis he can use this acquired information as he wishes, to, for
example, understand the harmony of that piece, find other similar
songs, or write an accompaniment. Finding the chords of a piece of
tonal music is just metadata extraction, and the study of chord
finding as such can increse understanding of the performance and
limitations of current techniques.

The problem of automated finding the chord structure of symbolic
scores has been approached from many directions, mostly mirroring
approaches to natural language processing currently in fashion.
First, Winograd \cite{winograd:linguistics} and Ulrich
\cite{ulrich:analysis} developed backtracking parsers based of formal
grammars and productions. After a hiatus they were followed by
Maxwell's \cite{maxwell:expert} production rule-based expert system
and Temperley and Sleator's Melisma \cite{temperley.ea:modeling},
based on preference rules. These were followed by Pardo and
Birmingham's HarmAn \cite{pardo.ea:automated}, Barthelemy and Bonardi
\cite{barthelemy.ea:figured} and Taube's workbench
\cite{taube:automatic}, all built using pattern-matching as their core
chord-finding method. Following these are Tsui's \cite{tsui:harmonic}
neural network algorithm and Temperley's \cite{temperley:bayesian}
bayesian approach.

All these techniques and methods were described informally, and very
little source code is avaliable for testing. Some articles, most
notably Barthelemy and Bonardi \cite{barthelemy.ea:figured} and
Temperley and Sleator \cite{temperley.ea:modeling}, don't provide
enough information to reproduce their results reliably. Every
benchmark found in literature \cite{pardo.ea:automated,
  barthelemy.ea:figured, tsui:harmonic, taube:automatic,
  illescas.ea:harmonic} is only based on published examples, and this
difficults a thorough statistical evaluation of each technique's main
merits and flaws.

Our goal in this article is to frame some known and new approaches to
automatic chord finding into a more systematic Information Retrieval
framework, and use insights obtained in this process to evaluate and
improve their performance. We review Pardo and Birmingham's HarmAn
\cite{pardo.ea:automated}, Maxwell's expert system
\cite{maxwell:expert}, a version of Tsui's artificial neural networks
\cite{tsui:harmonic} and original algorithms based on decision trees
and k-nearest-neighbors. These algorithms are evaluated according to
their precision, recall, bias and variance in extracting the correct
chords by type. We also study the effect of incorporating enharmonic
information as input to our algorithms. Each algorithm is evaluated
against our corpus of 140 Bach chorales from the Riemenschneider
\cite{bach:371} edition.

In this article we first discuss the main characteristics and behavior
of each algorithm. In section \ref{sec:heuristic-algorithms} we
present Pardo et al and Maxwell's algorithms, and discuss possible
enhancements to their original proposals. In section
\ref{sec:stat-algor} we analyze Tsui's artificial neural networks, our
decision trees and a baseline k-nearest-neighbours method. We compare
their overall performance in section \ref{sec:discussion} and state
our case in section \ref{sec:conclusions}.


\section{Heuristic algorithms}
\label{sec:heuristic-algorithms}

The defining characteristic of a heuristic algorithm is that its
behavior is based on rules of some form manually coded by an expert in
the problem domain. These rules might take the form of preference
rules (``prefer doing X over doing Y when applicable''), simple
pattern matching (``when a pattern somewhat like X is seen, do Y''),
production rules (``if X then do Y''), grammar-based formalisms (``X
can be classified as a Y if it can be generated by expanding Y's
definition''), first-order logic (``for all X belonging to Y there is
a Z such as X and Z are W''), and many other early AI
algorithms. These rules are also usually coupled with a searching
strategy to more quickly determine the best possible match in the
usually huge solution space.

Systems based on such techniques can be highly effective, condensing
the knowledge and understanding of human experts on the subject matter
and serving as useful aid in a decision process. However, they can
not, in the general case, easily adapt to different situations or
explore new solutions to the problem at hand. They also tend to be
brittle, failing in unforeseen corner cases. As many of them
fundamentally a guided search for a solution in a vast space they can
easily present performance problems when scaling to inputs of larger
sizes, although powerful heuristics can solve this problem by pruning
the space. \note{passos: seria interessante citar alguém aqui? esses
  parágrafos estão muito soltos?}

\subsection{Pardo and Birmingham's algorithm}
\label{sec:pardo}

Pardo and Birmingham \cite{pardo.ea:algorithms} describe a
pattern-matching algorithm for chordal analysis based on
templates. The article enumerates six such templates, and the
algorithm searches across the piece for the labeling that better
matches the notes found.

Some limiations are obvious from the article. The algorithm, for
example, has no notion of a minor chord with a minor seventh, or an
augmented chord, or a chord without a third (and, consequently, having
a melodic purpose on the piece). Also, since their system ignores
enharmonic information some fine distinctions are lost, and a german
augmented sixth is shown being classified as a dominant seventh.

We have extended the algorithm presented in the article by
incorporating more chord templates (giving a total of 10 chord
templates) and giving it access to enharmonic information, which
improved the recall of fully diminished chords and the precision in
recognizing major chords. We have also added a bass note detection.

\subsection{Maxwell's expert system}
\label{sec:maxwell}



\section{Machine Learning algorithms}
\label{sec:stat-algor}

Another category of approaches to harmonic analysis is comprised of
the machine learning algorithms. These techniques, originally from
statistics and pattern recognition, work by automatically extracting
patterns from the data and using these patterns to approximate a given
example classification. Some algorithms are deceptively simple in
their inner workings, but, they can be more effective than some
complex cognitive approaches \cite{gomez.ea:estimating}.

As these techniques are highly dependent on the number and
distribution of the independent parameters in the input data, some
preprocessing is usually necessary to extract significant features
that may be useful in the classification process. For example, a digit
recognizing program might first extract the edges of the image and use
their lengths and angles to determine which character it might
represent, or a text classifier might extract the most meaningful
words in a document and base its decision on that.

Feature extraction is also a part of the process where it might be
possible to preserve some invariance known a priori. For example, our
feature extractors for the k-nearest-neighbors and neural network
algorithms are invariant under transposition of every note in a given
sonority.

The choice of which features to extract and how to extract them has a
big impact on the efficiency of the algorithm \cite{mitchell:machine}.

\subsection{Decision Trees}
\label{sec:tree}

Decision trees are popular classifiers. They work by considering each
instance as a collection of attributes, and representing a
classification as a tree of decisions. In our implementation each
attribute has one of a discrete set of values. The tree is built
top-down, at each time considering the attribute on which a
classification decision would be better. Then, for each possible value
for that attribute a new subtree is built using the same method, until
no more attributes are left, and a classification has to be
made. Decision trees classify an instance by looking, starting from
the root of the tree, for the most similar path to the
instance. \note{consertar, muito feio}

As this work is based on Bach Chorales and every chorale has four
voices, the features considered were the four pitches in a sonority,
from bass to soprano. This approach, however, does not scale very well
to other musical styles, and a modification of our decision tree
library (originally code from \cite{mitchell:machine}) to better
handle numerical attributes is necessary for the improvement of this
algorithm. \note{feio, reescrever}

\subsection{Artificial Neural Networks}
\label{sec:neural-net}

Artificial neural networks are a family of useful non-linear
regression and classification tools. Their unifying characteristic is
that computation happens by having artificial neurons exchange
messages in the form of activation values. Some special neuron's
actiavtion values are then chosen as the output of the neural
network. Our algorithms are multilayer feedforward perceptrons, which
have a layered structure.

The artificial neural networks we used are modeled after Tsui's Root
Network A \cite{tsui:harmonic}. The only difference is in the feature
extraction process. Tsui \cite{tsui:harmonic} uses as features a
vector, each position representing one pitch class. On training, he
first transposes every sonority to the 12 possible pitch classes to
ensure that the network is transposition-invariant. While interesting,
this approach becomes computationally prohibitive if one wants to
distinguish enharmonic notes. Instead, we chose to transpose each
sonority so that it has C as its bass note. This codification has
interesting properties and has no accuracy burden.

\subsection{K-Nearest-Neighbors}
\label{sec:knn}

The k-nearest-neighbors algorithm is one of the simplest techniques
used in Machine Learning (along with the naive bayes classifier and
decision trees). It represents each training input as a point in a
multidimensional space and classifies each input by choosing the
classification most similar to the k nearest training points. As
simple as it may seem, the k-nearest-neighbor algorithm is guaranteed,
on sufficient data points, to yield an error rate at most twice the
maximum possible given the distribution of the data
\cite{fix.ea:important}.

The features used in this classifier were the same as in the neural
networks. This classifier was designed and implemented to serve as a
baseline to our other algorithms, that are expected to detect more
useful patterns in the data.


\section{Discussion}
\label{sec:discussion}



\section{Conclusions}
\label{sec:conclusions}


\bibliographystyle{plain}
\bibliography{strings-short,ismir,programs,coding,harmonic-analysis,dont-have,artifical-inteligence,music-harmony-and-theory,licenses,icmc}

\end{document}

